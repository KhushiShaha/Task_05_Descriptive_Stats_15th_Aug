## Task 05 - Advanced Prompt Engineering & LLM Evaluation

This submission builds upon the foundational work completed in the first reporting period. It expands the prompt set to include more advanced, strategic, and simulation-based questions to evaluate the reasoning capabilities of large language models like ChatGPT using SU Women’s Lacrosse data.

## Summary

Introduced **advanced analytical prompts** beyond descriptive stats.
Focused on evaluating **player efficiency**, **defensive impact**, and **scenario-based hypotheticals**.
Designed prompts that challenge the LLM’s ability to simulate, infer, and reason using derived metrics.

## Files Included

`Prompts.txt` – Contains 7 upgraded LLM prompts.
`README.md` - Summary
Scripts reused from previous submission.

## Notes

Prompts are meant to be tested in isolation or as part of an evaluation pipeline. Answers can be validated with prior Python scripts that calculate player summaries and performance indices.
